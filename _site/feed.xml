<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-06-27T21:47:53+05:30</updated><id>/feed.xml</id><title type="html">Ashish Kumar Sinha</title><subtitle>Let's discuss Data</subtitle><entry><title type="html">Negative Sampling</title><link href="/2021/06/16/my-first-blog-post.html" rel="alternate" type="text/html" title="Negative Sampling" /><published>2021-06-16T11:15:31+05:30</published><updated>2021-06-16T11:15:31+05:30</updated><id>/2021/06/16/my-first-blog-post</id><content type="html" xml:base="/2021/06/16/my-first-blog-post.html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Say you are training a NLP model with 100s of millions of tokens in your vocabulary. Let us assume for the sake of simplicity that the NLP model is a bigram model. &lt;br /&gt;
To end goal of a bigram model is to find the conditional probability that the token t&lt;sub&gt;1&lt;/sub&gt; will appear next to token t&lt;sub&gt;0&lt;/sub&gt; i.e the probability P(t&lt;sub&gt;1&lt;/sub&gt;|t&lt;sub&gt;0&lt;/sub&gt;)&lt;/p&gt;</content><author><name></name></author><summary type="html">Introduction</summary></entry></feed>